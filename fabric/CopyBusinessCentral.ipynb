{"cells":[{"cell_type":"markdown","id":"ae53e9bf-8787-4d07-b709-d896fd16cc5f","metadata":{"editable":false,"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"source":["## Business Central merge data notebook\n","In this part the files in the delta folder will be merge with the Lakehouse table.\n","- It iterates first on the folders to append to the existing table.\n","- After that is will remove all duplicates by sorting the table. \n","- At last it will remove all deleted records inside the table that are deleted in Business Central\n","\n","Please change the parameters in the first part."]},{"cell_type":"code","execution_count":null,"id":"34dc5721-e317-4dc0-88ef-2c6bafb494da","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.6812441Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:06.8530455Z\",\"execution_finish_time\":\"2023-08-15T09:15:07.1828235Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["%%pyspark\n","# settings\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\",\"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizewrite.enabled\",\"true\")\n","spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n","spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n","spark.conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n","spark.conf.set(\"spark.sql.delta.commitProtocol.enabled\", \"true\")\n","\n","# file paths\n","folder_path_spark = 'Files/deltas/' # this is mostly the default\n","folder_path_json = '/lakehouse/default/Files/' # this is mostly the default\n","folder_path = '/lakehouse/default/Files/deltas/' # this is mostly the default\n","\n","# parameters\n","workspace = 'fabricTest' #can also be a GUID\n","Lakehouse = 'businessCentral'; #can also be a GUID\n","Remove_delta = True; #will remove the delta files if everything is processed\n","Drop_table_if_mismatch = False; #option to drop the table if json file has different columns then in the table\n","no_Partition = 258 #how many partition is used in the dataframe, a good starting point might be 2-4 partitions per CPU core in your Spark cluster"]},{"cell_type":"code","execution_count":null,"id":"33ddc3d7","metadata":{},"outputs":[],"source":["%%pyspark\n","import json\n","import os\n","import glob\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import desc\n","\n","for entry in os.scandir(folder_path):\n"," if entry.is_dir():\n","    if glob.glob(folder_path + entry.name + '/*'):\n","        table_name = entry.name.replace(\"-\",\"\")\n","\n","        df_new = spark.read.option(\"minPartitions\", no_Partition).format(\"csv\").option(\"header\",\"true\").load(folder_path_spark + entry.name +\"/*\")   \n","        f = open(folder_path_json + entry.name +\".cdm.json\")\n","        schema = json.load(f)\n","        # Parse the schema to get column names and data types\n","        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n","        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n","        for col_name, col_type in zip(column_names, column_types):\n","            if col_type == \"String\":\n","                col_type = \"string\"\n","            if col_type == \"Guid\":\n","                col_type = \"string\"\n","            if col_type == \"Code\":\n","                col_type = \"object\"\n","            if col_type == \"Option\":\n","                col_type = \"string\"\n","            if col_type == \"Date\":\n","                col_type = \"date\"\n","            if col_type == \"Time\":\n","                col_type = \"string\"\n","            if col_type == \"DateTime\":\n","                col_type = \"date\"\n","            if col_type == \"Duration\":\n","                col_type = \"timedelta\"\n","            if col_type == \"Decimal\":\n","                col_type = \"float\"\n","            if col_type == \"Boolean\":\n","                col_type = \"boolean\"\n","            if col_type == \"Integer\":\n","                col_type = \"int\"\n","            if col_type == \"Int64\":\n","                col_type = \"int\"\n","            if col_type == \"Int32\":\n","                col_type = \"int\"\n","\n","            df_new = df_new.withColumn(col_name, df_new[col_name].cast(col_type))\n","\n","        #check if the table excists\n","        if table_name in [t.name for t in spark.catalog.listTables()]:  \n","            #read the old data into a new dataframe and union with the new dataframe\n","            SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;  \n","            df_old = spark.sql(SQL_Query)\n","            df_new = df_new.union(df_old).repartition(no_Partition)\n","\n","            #delete all old records\n","            df_deletes = df_new.filter(df_new['SystemCreatedAt-2000000001'].isNull())\n","            df_new = df_new.join(df_deletes, [\"systemId-2000000000\"], \"leftanti\")\n","\n","            # remove duplicates by filtering on systemID and systemModifiedAt fields\n","            df_new = df_new.orderBy('systemId-2000000000',desc('SystemModifiedAt-2000000003'))\n","            df_new = df_new.dropDuplicates(['systemId-2000000000'])\n","\n","            #overwrite the dataframe in the new table\n","            df_new.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name) \n","        else:  \n","            #table isn't there so just insert it\n","            df_new.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","        #delete the files\n","        if Remove_delta:  \n","            files = glob.glob(folder_path + entry.name + \"/*\")  \n","            for f in files:\n","                os.remove(f)"]},{"cell_type":"code","execution_count":null,"id":"0594c099-6512-4777-82e2-9a3a058512fe","metadata":{"cellStatus":"{\"MOD Administrator\":{\"queued_time\":\"2023-08-15T09:15:05.7249665Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-08-15T09:15:07.7601315Z\",\"execution_finish_time\":\"2023-08-15T09:15:18.128035Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}","collapsed":false,"editable":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false}},"outputs":[],"source":["%%pyspark\n","import json\n","import os\n","import glob\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import desc\n","\n","for entry in os.scandir(folder_path):\n"," if entry.is_dir():\n","    if glob.glob(folder_path + entry.name + '/*'):\n","        table_name = entry.name.replace(\"-\",\"\")\n","\n","        df_new = spark.read.format(\"csv\").option(\"header\",\"true\").load(folder_path_spark + entry.name +\"/*\")   \n","        \n","        f = open(folder_path_json + entry.name +\".cdm.json\")\n","        schema = json.load(f)\n","        # Parse the schema to get column names and data types\n","        column_names = [attr[\"name\"] for attr in schema[\"definitions\"][0][\"hasAttributes\"]] \n","        column_types = [attr['dataFormat'] for attr in schema[\"definitions\"][0][\"hasAttributes\"]]   \n","        for col_name, col_type in zip(column_names, column_types):\n","            if col_type == \"String\":\n","                col_type = \"string\"\n","            if col_type == \"Guid\":\n","                col_type = \"string\"\n","            if col_type == \"Code\":\n","                col_type = \"object\"\n","            if col_type == \"Option\":\n","                col_type = \"string\"\n","            if col_type == \"Date\":\n","                col_type = \"date\"\n","            if col_type == \"DateTime\":\n","                col_type = \"date\"\n","            if col_type == \"Time\":\n","                col_type = \"string\"\n","            if col_type == \"Duration\":\n","                col_type = \"timedelta\"\n","            if col_type == \"Decimal\":\n","                col_type = \"float\"\n","            if col_type == \"Boolean\":\n","                col_type = \"boolean\"\n","            if col_type == \"Integer\":\n","                col_type = \"int\"\n","            if col_type == \"Int64\":\n","                col_type = \"int\"\n","            if col_type == \"Int32\":\n","                col_type = \"int\"\n","\n","            df_new = df_new.withColumn(col_name, df_new[col_name].cast(col_type))\n","\n","        #check if the table excists\n","        if table_name in [t.name for t in spark.catalog.listTables()]:  \n","            #read the old data into a new dataframe and union with the new dataframe\n","            SQL_Query = \"SELECT * FROM \" + Lakehouse +\".\"+table_name;  \n","            df_old = spark.sql(SQL_Query)\n","            df_new = df_new.union(df_old)\n","\n","            #delete all old records\n","            df_deletes = df_new.filter(df_new['SystemCreatedAt-2000000001'].isNull())\n","            df_new = df_new.join(df_deletes, [\"systemId-2000000000\"], \"leftanti\")\n","\n","            # remove duplicates by filtering on systemID and systemModifiedAt fields\n","            df_new = df_new.orderBy('systemId-2000000000',desc('SystemModifiedAt-2000000003'))\n","            df_new = df_new.dropDuplicates(['systemId-2000000000'])\n","\n","            #overwrite the dataframe in the new table\n","            df_new.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name) \n","        else:  \n","            #table isn't there so just insert it\n","            df_new.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","        #delete the files\n","        if Remove_delta:  \n","            files = glob.glob(folder_path + entry.name + \"/*\")  \n","            for f in files:\n","                os.remove(f)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"state":{},"token":"a69b4b72-86b0-4373-b695-ef01cd53bbb1"},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","known_lakehouses":"[{\"id\":\"9fbacb3e-d0df-43a4-814b-abe4cb623a81\"}]"}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"9fbacb3e-d0df-43a4-814b-abe4cb623a81","default_lakehouse_name":"businessCentral","default_lakehouse_workspace_id":"21a92229-a0fb-4256-86bd-4b847b8006ed","known_lakehouses":[{"id":"9fbacb3e-d0df-43a4-814b-abe4cb623a81"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
